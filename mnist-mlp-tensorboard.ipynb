{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard with MNIST MLP\n",
    "#### Debugging a neural network with tensorboard\n",
    "Try playing with the neural network hyperparameters such as learning rate, batch size, activation functions and see what happens to the Tensorboard output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizes of Input, Output and Hidden Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 784\n",
    "n_outputs = 10\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output Placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs], name=\"Batch_X\")\n",
    "batch_y = tf.placeholder(dtype=tf.float32, shape=[None, n_outputs], name=\"Batch_Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensboard summary ops\n",
    "Tensorboard provides summary operation that you can apply onto tensors to visualize them over training steps. \n",
    "Few of them are listed below:\n",
    "1. tf.summary.scalar(): For scalars such as loss, accuracy. \n",
    "2. tf.summary.image(): For image matrices such as a 28x28 MNIST input image.\n",
    "3. tf.summary.histogram(): For numeric distributions used for weights, biases, activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh1i = tf.Variable(tf.random_normal((n_inputs, n_hidden_1)), dtype=tf.float32, name=\"Wh1\")\n",
    "tf.summary.histogram('Wh1', Wh1i)\n",
    "bh1 = tf.Variable(tf.random_normal((1, n_hidden_1)), dtype=tf.float32, name=\"Bh1\")\n",
    "tf.summary.histogram('Bh1', bh1)\n",
    "\n",
    "Wh2h1 = tf.Variable(tf.random_normal((n_hidden_1, n_hidden_2)), dtype=tf.float32, name=\"Wh2\")\n",
    "tf.summary.histogram('Wh2h1', Wh2h1)\n",
    "bh2 = tf.Variable(tf.random_normal((1, n_hidden_2)), dtype=tf.float32, name=\"Bh2\")\n",
    "tf.summary.histogram('Bh2', bh2)\n",
    "\n",
    "Woh2 = tf.Variable(tf.random_normal((n_hidden_2, n_outputs)), dtype=tf.float32, name=\"Wo\")\n",
    "tf.summary.histogram('Woh2', Woh2)\n",
    "bo = tf.Variable(tf.random_normal((1, n_outputs)), name=\"Bo\")\n",
    "tf.summary.histogram('Bo', bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name scopes\n",
    "Tensorboard can generate a visual computation graph output for the graph that you construct with tensorflow. But the graph that is created by default won't make much sense. You can try it by adding just these two lines to the end of the session.\n",
    "1. writer = tf.summary.FileWriter(\"/path/to/save/\")\n",
    "2. writer.add_graph(sess.graph)\n",
    "\n",
    "In order to better visualize the graph you can scope related operations into semantic layers which you will see below. To do this you need to use tf.name_scope(). \n",
    "\n",
    "You can also supply name parameter to the desired tensor ops, variables, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "We are using ReLU as activation function and softmax at the final layer to get max probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_1\"):\n",
    "    zh1 = tf.add(tf.matmul(batch_x, Wh1i), bh1)\n",
    "    # ah1 = tf.nn.tanh(zh1)\n",
    "    # ah1 = tf.nn.relu(zh1)\n",
    "    # ah1 = tf.nn.sigmoid(zh1)\n",
    "    ah1 = zh1\n",
    "\n",
    "with tf.name_scope(\"Layer_2\"):\n",
    "    zh2 = tf.add(tf.matmul(ah1, Wh2h1), bh2)\n",
    "    # ah2 = tf.nn.tanh(zh2)\n",
    "    # ah2 = tf.nn.relu(zh2)\n",
    "    # ah2 = tf.nn.sigmoid(zh2)\n",
    "    ah2 = zh2\n",
    "\n",
    "with tf.name_scope(\"Layer_Output\"):\n",
    "    zo = tf.add(tf.matmul(ah2, Woh2), bo)\n",
    "    # ao = tf.nn.tanh(zo)\n",
    "    # ao = tf.nn.relu(zo)\n",
    "    # ao = tf.nn.sigmoid(zo)\n",
    "    ao = zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = ao\n",
    "prediction = tf.nn.softmax(ao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=batch_y))\n",
    "    tf.summary.scalar('Loss', loss_op)\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of training_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "display_step = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "writer = tf.summary.FileWriter(\"/tmp/mnist_demo/3\") # change the path everytime you rerun the session to properly see the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # tensorboard settings\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            x, y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            s, _, c = sess.run([merged_summary, train_op, loss_op], feed_dict={batch_x: x, \n",
    "                                                         batch_y: y})\n",
    "            writer.add_summary(s, i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "#     pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    with tf.name_scope(\"Test_accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(batch_y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.summary.scalar('Accuracy', accuracy)\n",
    "    print(\"Accuracy:\", accuracy.eval({batch_x: mnist.test.images, batch_y: mnist.test.labels}))\n",
    "    \n",
    "    # save the trained model\n",
    "    save_path = saver.save(sess, \"/tmp/model-mnist.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Tensorboard Output\n",
    "To run tensorboard you can do:\n",
    "python -m tensorboard.main --logdir=\"/path/to/save/file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "start = random.randint(0, mnist.test.num_examples - n_images)\n",
    "test_images = mnist.test.images[start:start + n_images - 1]\n",
    "test_labels = mnist.test.labels[start:start + n_images - 1]\n",
    "for idx, image in enumerate(test_images):\n",
    "    image = np.reshape(image, (28, 28))\n",
    "    print(np.argmax(test_labels[idx]), end=',')\n",
    "    plt.subplot(1,n_images,idx+1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model-mnist.ckpt\")\n",
    "    _logits = sess.run(logits, feed_dict={batch_x: test_images})\n",
    "    _pred = tf.nn.softmax(_logits)\n",
    "    predicted_number = tf.argmax(_pred, 1)\n",
    "    _predicted_number = predicted_number.eval(feed_dict={batch_x: test_images})\n",
    "    _actual_number = np.argmax(test_labels, 1)\n",
    "    print('Predicted:',_predicted_number,'Actual:',_actual_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
